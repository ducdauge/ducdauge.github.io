[{"authors":["emponti"],"categories":null,"content":"I am an assistant professor in Natural Language Processing at the University of Edinburgh.\nIf you are a prospective PhD student or postdoc please visit the FAQ page for more information.\nI am currently leading two main projects:\nAdaptive Tokenization and Memory in Foundation Models for Efficient and Long-Horizon AI (AToM ⚛︎) funded by an ERC Starting Grant. This project aims at designing the next generation of foundation model architectures to reduce energy demand and carbon emissions, while enabling permanent memories, inference time hyper-scaling, and long-horizon world modelling. Tracking Evolving AI and Systems (TEAS ☕︎), which aims at benchmarking cost–accuracy–performance trade-offs in existing hardware and devising analytical tools to extrapolate performance to new hardware. My research focuses on:\nadaptive memory and tokenization in foundation models (see my NeurIPS 2024 tutorial on dynamic sparsity): I aim to redefine the units of computation of foundation models by adaptively compressing the sequences of their hidden representations and memory. This allows models to tokenize raw, modality-agnostic data end-to-end, learning hierarchical abstractions. Simultaneously, it provides the foundations for permanent model memories and inference-time hyper-scaling.\nmodular deep learning: I am interested in designing neural architectures that route information to specialised modules (e.g., sparse subnetworks). This facilitates systematic generalisation and conditional computation.\ncomputational typology: I wish to understand how languages vary, across the world and its cultures, within a computational framework. Multimodal models in particular give us an powerful tool to study how form depends on grounded, embodied representations of meaning and function.\nI spent the last year as a visiting professor at NVIDIA. Previously, I was a visiting postdoctoral scholar at Stanford University and a postdoctoral fellow in computer science at Mila - Quebec AI Institute in Montreal. In 2021, I obtained a PhD from the University of Cambridge, St John\u0026rsquo;s College. Once upon a time I studied modern literature at the University of Pavia. Deep in my heart, I am still a humanist: some of my favourite writers are Italo Calvino, Ursula Le Guin, and Titus Lucretius Carus.\nMy research has been featured on the Economist and Scientific American, among others. it is currently supported by ERC, ARIA, and various gifts/compute from Google DeepMind, NVIDIA, and NatWest. I also received a Google Research Faculty Award, and Best Paper / SAC Highlight Awards at ACL, EMNLP, and RepL4NLP. I am a board member of SIGTYP, the ACL special interest group for computational typology, a Scholar of the European Lab for Learning and Intelligent Systems (ELLIS), and part of the TACL journal editorial team.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"213193b58a0bdd6832138a4f61012d29","permalink":"https://ducdauge.github.io/authors/emponti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/emponti/","section":"authors","summary":"I am an assistant professor in Natural Language Processing at the University of Edinburgh.\nIf you are a prospective PhD student or postdoc please visit the FAQ page for more information.\nI am currently leading two main projects:\nAdaptive Tokenization and Memory in Foundation Models for Efficient and Long-Horizon AI (AToM ⚛︎) funded by an ERC Starting Grant. This project aims at designing the next generation of foundation model architectures to reduce energy demand and carbon emissions, while enabling permanent memories, inference time hyper-scaling, and long-horizon world modelling.","tags":null,"title":"Edoardo M. Ponti","type":"authors"},{"authors":null,"categories":null,"content":"See the model on Hugging Face: https://huggingface.co/allenai/Bolmo-7B\n","date":1767225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1767225600,"objectID":"a63bab28bee877d36731e094182841c8","permalink":"https://ducdauge.github.io/project/bolmo-7b/","publishdate":"2026-01-01T00:00:00Z","relpermalink":"/project/bolmo-7b/","section":"project","summary":"State-of-the-art, fully open-source large language model with latent tokenization. Available in 1B and 7B sizes.","tags":null,"title":"allenai/Bolmo-7B","type":"project"},{"authors":null,"categories":null,"content":"See the model on Hugging Face: https://huggingface.co/nvidia/Qwen3-8B-DMS-8x\n","date":1767225600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1767225600,"objectID":"2f96575f1b00d84a967adc6122a0d35a","permalink":"https://ducdauge.github.io/project/qwen-8b-dms-8x/","publishdate":"2026-01-01T00:00:00Z","relpermalink":"/project/qwen-8b-dms-8x/","section":"project","summary":"8x KV cache compression without quality degradation. Ideal for inference-time scaling.","tags":["atom","atom-model","huggingface"],"title":"nvidia/Qwen3-8B-DMS-8x","type":"project"},{"authors":["Benjamin Minixhofer","Edoardo M. Ponti","Ivan Vulić"],"categories":null,"content":"","date":1715558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715558400,"objectID":"62947137ab9cb7b52473445f6cd50254","permalink":"https://ducdauge.github.io/publication/2024-zett/","publishdate":"2024-05-13T00:00:00Z","relpermalink":"/publication/2024-zett/","section":"publication","summary":"Language models (LMs) are bound to their tokenizer, which maps raw text to a sequence of vocabulary items (tokens). This restricts their flexibility: for example, LMs trained primarily on English may still perform well in other natural and programming languages, but have vastly decreased efficiency due to their English-centric tokenizer. To mitigate this, we should be able to swap the original LM tokenizer with an arbitrary one, on the fly, without degrading performance. Hence, in this work we define a new problem: Zero-Shot Tokenizer Transfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for the tokens in the vocabulary of the new tokenizer. Since prior heuristics for initializing embeddings often perform at chance level in a ZeTT setting, we propose a new solution: we train a hypernetwork taking a tokenizer as input and predicting the corresponding embeddings. We empirically demonstrate that the hypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and decoder LLMs (e.g., Mistral-7B). Our method comes close to the original models' performance in cross-lingual and coding tasks while markedly reducing the length of the tokenized sequence. We also find that the remaining gap can be quickly closed by continued training on less than 1B tokens. Finally, we show that a ZeTT hypernetwork trained for a base (L)LM can also be applied to fine-tuned variants without extra training. Overall, our results make substantial strides toward detaching LMs from their tokenizer.","tags":null,"title":"Zero-Shot Tokenizer Transfer","type":"publication"},{"authors":["Piotr Nawrot","Adrian Łańcucki","Marcin Chochowski","David Tarjan","Edoardo M. Ponti"],"categories":null,"content":"","date":1710374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710374400,"objectID":"36b7dd06cf50ab20a9f192eb1e4ddd82","permalink":"https://ducdauge.github.io/publication/2024-dmc/","publishdate":"2024-03-14T00:00:00Z","relpermalink":"/publication/2024-dmc/","section":"publication","summary":"Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for on-line key-value cache compression at inference time. Most importantly, the model learns to apply different compression rates in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to ~3.7x throughput increase in auto-regressive inference on a NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. We find that DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention (GQA). GQA and DMC can be even combined to obtain compounded gains. As a result DMC fits longer contexts and larger batches within any given memory budget.","tags":null,"title":"Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference","type":"publication"},{"authors":null,"categories":null,"content":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"53e892b8b41cc4caece1cfd5ef21d6e7","permalink":"https://ducdauge.github.io/license/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/license/","section":"","summary":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.","tags":null,"title":"LICENSE: CC-BY-SA","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6b0daa868220d589619c8dd2ddb2baa2","permalink":"https://ducdauge.github.io/atom/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/atom/","section":"","summary":"","tags":null,"title":"ATOM","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"db9c7cbc7ea39f900c5763526a87142d","permalink":"https://ducdauge.github.io/faq/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/faq/","section":"","summary":"","tags":null,"title":"Frequently Asked Questions","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"65de3680a280f6bf29dc34fe1adad5a6","permalink":"https://ducdauge.github.io/talks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talks/","section":"","summary":"","tags":null,"title":"Talks","type":"widget_page"}]