[{"authors":["emponti"],"categories":null,"content":"I am a third-year Ph.D. student in Computational Linguistics at the University of Cambridge, where I am affiliated to St John\u0026rsquo;s College. I am supervised by Prof. Anna Korhonen and funded through the ERC project LEXICAL. I have previously interned as an AI/ML researcher at Apple in Cupertino, and won a Google Research Faculty Award for a project co-written with my supervisor. I am a frequent collaborator of Ryan Cotterell\u0026rsquo;s lab Rycolab at ETH Zürich.\nMost of my research revolves around the inductive bias of artificial neural networks towards language, in order to enable few-shot learning. Moreover, I am interested in integrating text-based word representations with world and lexical knowledge.\nI am a literature enthusiast, a decently bad violinist and a cat lover.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"213193b58a0bdd6832138a4f61012d29","permalink":"https://ducdauge.github.io/authors/emponti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/emponti/","section":"authors","summary":"I am a third-year Ph.D. student in Computational Linguistics at the University of Cambridge, where I am affiliated to St John\u0026rsquo;s College. I am supervised by Prof. Anna Korhonen and funded through the ERC project LEXICAL. I have previously interned as an AI/ML researcher at Apple in Cupertino, and won a Google Research Faculty Award for a project co-written with my supervisor. I am a frequent collaborator of Ryan Cotterell\u0026rsquo;s lab Rycolab at ETH Zürich.","tags":null,"title":"Edoardo M. Ponti","type":"authors"},{"authors":["Edoardo M. Ponti","Goran Glavaš","Olga Majewska","Qianchu Liu","Ivan Vulić","Anna Korhonen"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"6f3187d13bb16b0da61dabe9c81ae1b4","permalink":"https://ducdauge.github.io/publication/2020-xcopa/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/2020-xcopa/","section":"publication","summary":"In order to simulate human language capacity, natural language processing systems must complement the explicit information derived from raw text with the ability to reason about the possible causes and outcomes of everyday situations. Moreover, the acquired world knowledge should generalise to new languages, modulo cultural differences. Advances in machine commonsense reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages. We benchmark a range of state-of-the-art models on this novel dataset, revealing that current methods based on multilingual pretraining and zero-shot fine-tuning transfer suffer from the curse of multilinguality and fall short of performance in monolingual settings by a large margin. Finally, we propose ways to adapt these models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. XCOPA is available at github.com/cambridgeltl/xcopa.","tags":null,"title":"XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning","type":"publication"},{"authors":["Ivan Vulić","Simon Baker","Edoardo Maria Ponti","Ulla Petti","Ira Leviant","Kelly Wing","Olga Majewska","Eden Bar","Matt Malone","Thierry Poibeau","Roi Reichart","Anna Korhonen"],"categories":null,"content":"","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"5f016cdd2f71f4c105c50d0c4035c22d","permalink":"https://ducdauge.github.io/publication/2020-multi-simlex/","publishdate":"2020-03-10T00:00:00Z","relpermalink":"/publication/2020-multi-simlex/","section":"publication","summary":"We introduce Multi-SimLex, a large-scale lexical resource and evaluation benchmark covering datasets for 12 typologically diverse languages, including major languages (e.g., Mandarin Chinese, Spanish, Russian) as well as less-resourced ones (e.g., Welsh, Kiswahili). Each language dataset is annotated for the lexical relation of semantic similarity and contains 1,888 semantically aligned concept pairs, providing a representative coverage of word classes (nouns, verbs, adjectives, adverbs), frequency ranks, similarity intervals, lexical fields, and concreteness levels. Additionally, owing to the alignment of concepts across languages, we provide a suite of 66 cross-lingual semantic similarity datasets. Due to its extensive size and language coverage, Multi-SimLex provides entirely novel opportunities for experimental evaluation and analysis. On its monolingual and cross-lingual benchmarks, we evaluate and analyze a wide array of recent state-of-the-art monolingual and cross-lingual representation models, including static and contextualized word embeddings (such as fastText, M-BERT and XLM), externally informed lexical representations, as well as fully unsupervised and (weakly) supervised cross-lingual word embeddings. We also present a step-by-step dataset creation protocol for creating consistent, Multi-Simlex-style resources for additional languages. We make these contributions -- the public release of Multi-SimLex datasets, their creation protocol, strong baseline results, and in-depth analyses which can be be helpful in guiding future developments in multilingual lexical semantics and representation learning -- available via a website which will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages.","tags":null,"title":"Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual Lexical Semantic Similarity","type":"publication"},{"authors":["Edoardo M. Ponti","Ivan Vulić","Ryan Cotterell","Marinela Parovic","Roi Reichart","Anna Korhonen"],"categories":null,"content":"","date":1580342400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580342400,"objectID":"4e5771a86570d714cd83ed82c0c1b06f","permalink":"https://ducdauge.github.io/publication/2020-parameter-space/","publishdate":"2020-01-30T00:00:00Z","relpermalink":"/publication/2020-parameter-space/","section":"publication","summary":"Most combinations of NLP tasks and language varieties lack in-domain examples for supervised training because of the paucity of annotated data. How can neural models make sample-efficient generalizations from task-language combinations with available data to low-resource ones? In this work, we propose a Bayesian generative model for the space of neural parameters. We assume that this space can be factorized into latent variables for each language and each task. We infer the posteriors over such latent variables based on data from seen task-language combinations through variational inference. This enables zero-shot classification on unseen combinations at prediction time. For instance, given training data for named entity recognition (NER) in Vietnamese and for part-of-speech (POS) tagging in Wolof, our model can perform accurate predictions for NER in Wolof. In particular, we experiment with a typologically diverse sample of 33 languages from 4 continents and 11 families, and show that our model yields comparable or better results than state-of-the-art, zero-shot cross-lingual transfer methods; it increases performance by 4.49 points for POS tagging and 7.73 points for NER on average compared to the strongest baseline.","tags":null,"title":"Parameter Space Factorization for Zero-Shot Learning across Tasks and Languages","type":"publication"},{"authors":["Edoardo M. Ponti","Ivan Vulić","Goran Glavaš","Roi Reichart","Anna Korhonen"],"categories":null,"content":"","date":1572739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572739200,"objectID":"190e8248c4b2870af3a50f5a80ef3a4a","permalink":"https://ducdauge.github.io/publication/2019-crosslingual-specialization/","publishdate":"2019-11-03T00:00:00Z","relpermalink":"/publication/2019-crosslingual-specialization/","section":"publication","summary":"Semantic specialization integrates structured linguistic knowledge from external resources (such as lexical relations in WordNet) into pretrained distributional vectors in the form of constraints. However, this technique cannot be leveraged in many languages, because their structured external resources are typically incomplete or non-existent. To bridge this gap, we propose a novel method that transfers specialization from a resource-rich source language (English) to virtually any target language. Our specialization transfer comprises two crucial steps: 1) Inducing noisy constraints in the target language through automatic word translation; and 2) Filtering the noisy constraints via a state-of-the-art relation prediction model trained on the source language constraints. This allows us to specialize any set of distributional vectors in the target language with the refined constraints. We prove the effectiveness of our method through intrinsic word similarity evaluation in 8 languages, and with 3 downstream tasks in 5 languages: lexical simplification, dialog state tracking, and semantic textual similarity. The gains over the previous state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages.","tags":null,"title":"Cross-lingual Semantic Specialization via Lexical Relation Induction","type":"publication"},{"authors":["Edoardo M. Ponti","Ivan Vulić","Ryan Cotterell","Roi Reichart","Anna Korhonen"],"categories":null,"content":"","date":1572739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572739200,"objectID":"5159daa7fe88975a70852a5570d60987","permalink":"https://ducdauge.github.io/publication/2019-towards-zero/","publishdate":"2019-11-03T00:00:00Z","relpermalink":"/publication/2019-towards-zero/","section":"publication","summary":"Can we construct a neural language model which is inductively biased towards learning human language? Motivated by this question, we aim at constructing an informative prior for held-out languages on the task of character-level, open-vocabulary language modeling. We obtain this prior as the posterior over network weights conditioned on the data from a sample of training languages, which is approximated through Laplace’s method. Based on a large and diverse sample of languages, the use of our prior outperforms baseline models with an uninformative prior in both zero-shot and few-shot settings, showing that the prior is imbued with universal linguistic knowledge. Moreover, we harness broad language-specific information available for most languages of the world, ie, features from typological databases, as distant supervision for held-out languages. We explore several language modeling conditioning techniques, including concatenation and metanetworks for parameter generation. They appear beneficial in the few-shot setting, but ineffective in the zero-shot setting. Since the paucity of even plain digital text affects the majority of the world’s languages, we hope that these insights will broaden the scope of applications for language technology.","tags":null,"title":"Towards Zero-shot Language Modeling","type":"publication"},{"authors":["Anne Lauscher","Ivan Vulić","Edoardo Maria Ponti","Anna Korhonen","Goran Glavaš"],"categories":null,"content":"","date":1567641600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"85ca1918baa3c9f43dc60dc731adb6c9","permalink":"https://ducdauge.github.io/publication/2019-informing-unsupervised/","publishdate":"2019-09-05T00:00:00Z","relpermalink":"/publication/2019-informing-unsupervised/","section":"publication","summary":"Unsupervised pretraining models have been shown to facilitate a wide range of downstream applications. These models, however, still encode only the distributional knowledge, incorporated through language modeling objectives. In this work, we complement the encoded distributional knowledge with external lexical knowledge. We generalize the recently proposed (state-of-the-art) unsupervised pretraining model BERT to a multi-task learning setting: we couple BERT's masked language modeling and next sentence prediction objectives with the auxiliary binary word relation classification, through which we inject clean linguistic knowledge into the model. Our initial experiments suggest that our \"linguistically-informed\" BERT (LIBERT) yields performance gains over the linguistically-blind \"vanilla\" BERT on several language understanding tasks.","tags":null,"title":"Informing Unsupervised Pretraining with External Linguistic Knowledge","type":"publication"},{"authors":["Aishwarya Kamath","Jonas Pfeiffer","Edoardo Ponti","Goran Glavaš","Ivan Vulić"],"categories":null,"content":"","date":1564704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564704000,"objectID":"efe11af63f5d415ccd4afadd61bc73bb","permalink":"https://ducdauge.github.io/publication/2019-specializing-all/","publishdate":"2019-08-02T00:00:00Z","relpermalink":"/publication/2019-specializing-all/","section":"publication","summary":"Semantic specialization methods fine-tune distributional word vectors using lexical knowledge from external resources (e.g., WordNet) to accentuate a particular relation between words. However, such post-processing methods suffer from limited coverage as they affect only vectors of words seen in the external resources. We present the first postprocessing method that specializes vectors of all vocabulary words – including those unseen in the resources – for the asymmetric relation of lexical entailment (LE) (i.e., hyponymyhypernymy relation). Leveraging a partially LE-specialized distributional space, our POSTLE (i.e., post-specialization for LE) model learns an explicit global specialization function, allowing for specialization of vectors of unseen words, as well as word vectors from other languages via cross-lingual transfer. We capture the function as a deep feedforward neural network: its objective re-scales vector norms to reflect the concept hierarchy while simultaneously attracting hyponymyhypernymy pairs to better reflect semantic similarity. An extended model variant augments the basic architecture with an adversarial discriminator. We demonstrate the usefulness and versatility of POSTLE models with different input distributional spaces in different scenarios (monolingual LE and zero-shot cross-lingual LE transfer) and tasks (binary and graded LE). We report consistent gains over state-of-the-art LE-specialization methods, and successfully LE-specialize word vectors for languages without any external lexical knowledge.","tags":null,"title":"Specializing Distributional Vectors of All Words for Lexical Entailment","type":"publication"},{"authors":["Edoardo Maria Ponti","Helen O’Horan","Yevgeni Berzak","Ivan Vulić","Roi Reichart","Thierry Poibeau","Ekaterina Shutova","Anna Korhonen"],"categories":null,"content":"","date":1549929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549929600,"objectID":"cd6391dd003be2146672366475a2ea00","permalink":"https://ducdauge.github.io/publication/2019-modeling-language/","publishdate":"2019-02-12T00:00:00Z","relpermalink":"/publication/2019-modeling-language/","section":"publication","summary":"Linguistic typology aims to capture structural and semantic variation across the world’s languages. A large-scale typology could provide excellent guidance for multilingual Natural Language Processing (NLP), particularly for languages that suffer from the lack of human labeled resources. We present an extensive literature survey on the use of typological information in the development of NLP techniques. Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance. We show that this is due to both intrinsic limitations of databases (in terms of coverage and feature granularity) and under-utilization of the typological features included in them. We advocate for a new approach that adapts the broad and discrete nature of typological categories to the contextual and continuous nature of machine learning algorithms used in contemporary NLP. In particular, we suggest that such an approach could be facilitated by recent developments in data-driven induction of typological knowledge.","tags":null,"title":"Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"https://ducdauge.github.io/projects/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"See some of the projects I have worked on","tags":null,"title":"Projects","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"65de3680a280f6bf29dc34fe1adad5a6","permalink":"https://ducdauge.github.io/talks/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/talks/","section":"","summary":"Upcoming and recent talks / workshops","tags":null,"title":"Talks \u0026 Workshops","type":"widget_page"},{"authors":["Edoardo M. Ponti","Ivan Vulić","Goran Glavaš","Nikola Mrkšić","Anna Korhonen"],"categories":null,"content":"","date":1540598400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540598400,"objectID":"03ee6c487697bd49777c4adcbd6280f3","permalink":"https://ducdauge.github.io/publication/2018-adversarial-propagation/","publishdate":"2018-10-27T00:00:00Z","relpermalink":"/publication/2018-adversarial-propagation/","section":"publication","summary":"Semantic specialization is the process of fine-tuning pre-trained distributional word vectors using external lexical knowledge (eg, WordNet) to accentuate a particular semantic relation in the specialized vector space. While post-processing specialization methods are applicable to arbitrary distributional vectors, they are limited to updating only the vectors of words occurring in external lexicons (ie, seen words), leaving the vectors of all other words unchanged. We propose a novel approach to specializing the full distributional vocabulary. Our adversarial post-specialization method propagates the external lexical knowledge to the full distributional space. We exploit words seen in the resources as training examples for learning a global specialization function. This function is learned by combining a standard L2-distance loss with an adversarial loss: the adversarial component produces more realistic output vectors. We show the effectiveness and robustness of the proposed method across three languages and on three tasks: word similarity, dialog state tracking, and lexical simplification. We report consistent improvements over distributional word vectors and vectors specialized by other state-of-the-art specialization frameworks. Finally, we also propose a cross-lingual transfer method for zero-shot specialization which successfully specializes a full target distributional space without any lexical knowledge in the target language and without any bilingual data.","tags":null,"title":"Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word Vector Specialization","type":"publication"},{"authors":null,"categories":null,"content":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n  \n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"53e892b8b41cc4caece1cfd5ef21d6e7","permalink":"https://ducdauge.github.io/license/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/license/","section":"","summary":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.","tags":null,"title":"LICENSE: CC-BY-SA","type":"page"}]