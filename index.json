[{"authors":["andreas_grivas"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"220a01e35ec4b949be705aaa3097af3d","permalink":"https://ducdauge.github.io/authors/andreas_grivas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/andreas_grivas/","section":"authors","summary":"","tags":null,"title":"Andreas Grivas","type":"authors"},{"authors":["benjamin_minixhofer"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"933dc9733df35154ed249ff82533d5b2","permalink":"https://ducdauge.github.io/authors/benjamin_minixhofer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/benjamin_minixhofer/","section":"authors","summary":"","tags":null,"title":"Benjamin Minixhofer","type":"authors"},{"authors":["chentian_jiang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ac42b8bbee3ff4e9de5aa3253d0bab32","permalink":"https://ducdauge.github.io/authors/chentian_jiang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/chentian_jiang/","section":"authors","summary":"","tags":null,"title":"Chentian Jiang","type":"authors"},{"authors":["coleman_haley"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d24a07f093b26406967b8608e8179228","permalink":"https://ducdauge.github.io/authors/coleman_haley/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/coleman_haley/","section":"authors","summary":"","tags":null,"title":"Coleman Haley","type":"authors"},{"authors":["emile_van-krieken"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"28dbb12fa3758684198c5002c839f4bf","permalink":"https://ducdauge.github.io/authors/emile_van-krieken/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/emile_van-krieken/","section":"authors","summary":"","tags":null,"title":"Emile van Krieken","type":"authors"},{"authors":["emponti"],"categories":null,"content":"I am an assistant professor in Natural Language Processing at the University of Edinburgh and a visiting professor at NVIDIA.\nIf you are a prospective student please visit the FAQ page for more information.\nMy research focuses on:\nefficient architectures (see my NeurIPS 2024 tutorial on dynamic sparsity): I aim to redefine the units of computation of foundation models by adaptively compressing their intermediate representations and memory. This breaks models free from tokenizers and learn hierarchical abstractions over raw data.\nmodular deep learning: I am interested in designing neural architectures that route information to specialised modules (e.g., sparse subnetworks). This facilitates systematic generalisation and conditional computation.\ncomputational typology: I wish to understand how languages vary, across the world and its cultures, within a computational and mathematical framework. Multimodal models in particular give us an powerful tool to study how form depends on grounded, embodied representations of meaning and function.\nPreviously, I was a visiting postdoctoral scholar at Stanford University and a postdoctoral fellow in computer science at Mila - Quebec AI Institute in Montreal. In 2021, I obtained a PhD from the University of Cambridge, St John\u0026rsquo;s College. Once upon a time I studied typological and historical linguistics at the University of Pavia (deep in my heart, I am still a humanist).\nMy research has been featured on the Economist and Scientific American, among others. I received a Google Research Faculty Award and 2 Best Paper Awards at EMNLP 2021 and RepL4NLP 2019. I am a board member of SIGTYP, the ACL special interest group for computational typology, a Scholar of the European Lab for Learning and Intelligent Systems (ELLIS), and part of the TACL journal editorial team.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"213193b58a0bdd6832138a4f61012d29","permalink":"https://ducdauge.github.io/authors/emponti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/emponti/","section":"authors","summary":"I am an assistant professor in Natural Language Processing at the University of Edinburgh and a visiting professor at NVIDIA.\nIf you are a prospective student please visit the FAQ page for more information.\nMy research focuses on:\nefficient architectures (see my NeurIPS 2024 tutorial on dynamic sparsity): I aim to redefine the units of computation of foundation models by adaptively compressing their intermediate representations and memory. This breaks models free from tokenizers and learn hierarchical abstractions over raw data.","tags":null,"title":"Edoardo M. Ponti","type":"authors"},{"authors":["giwon_hong"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"36206d88c86d0598b8d7fd8c61ba3d46","permalink":"https://ducdauge.github.io/authors/giwon_hong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/giwon_hong/","section":"authors","summary":"","tags":null,"title":"Giwon Hong","type":"authors"},{"authors":["nina_gregorio"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2a066a32c3d9cd82189069839c90cff4","permalink":"https://ducdauge.github.io/authors/nina_gregorio/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/nina_gregorio/","section":"authors","summary":"","tags":null,"title":"Nina Gregorio","type":"authors"},{"authors":["osman_batur_ince"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9139c27357752d6e82c633ee5e203edc","permalink":"https://ducdauge.github.io/authors/osman_batur_ince/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/osman_batur_ince/","section":"authors","summary":"","tags":null,"title":"Osman Batur İnce","type":"authors"},{"authors":["piotr_nawrot"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0e767eda0cf9051e00521881806e52c3","permalink":"https://ducdauge.github.io/authors/piotr_nawrot/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/piotr_nawrot/","section":"authors","summary":"","tags":null,"title":"Piotr Nawrot","type":"authors"},{"authors":["yifu_qiu"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e017d71097667d8516b00358b51bb38b","permalink":"https://ducdauge.github.io/authors/yifu_qiu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yifu_qiu/","section":"authors","summary":"","tags":null,"title":"Yifu Qiu","type":"authors"},{"authors":["zeyu_huang"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8f0fc6a3a798a09490093e0eb3c231bc","permalink":"https://ducdauge.github.io/authors/zeyu_huang/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zeyu_huang/","section":"authors","summary":"","tags":null,"title":"Zeyu Huang","type":"authors"},{"authors":["Benjamin Minixhofer","Edoardo M. Ponti","Ivan Vulić"],"categories":null,"content":"","date":1715558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715558400,"objectID":"62947137ab9cb7b52473445f6cd50254","permalink":"https://ducdauge.github.io/publication/2024-zett/","publishdate":"2024-05-13T00:00:00Z","relpermalink":"/publication/2024-zett/","section":"publication","summary":"Language models (LMs) are bound to their tokenizer, which maps raw text to a sequence of vocabulary items (tokens). This restricts their flexibility: for example, LMs trained primarily on English may still perform well in other natural and programming languages, but have vastly decreased efficiency due to their English-centric tokenizer. To mitigate this, we should be able to swap the original LM tokenizer with an arbitrary one, on the fly, without degrading performance. Hence, in this work we define a new problem: Zero-Shot Tokenizer Transfer (ZeTT). The challenge at the core of ZeTT is finding embeddings for the tokens in the vocabulary of the new tokenizer. Since prior heuristics for initializing embeddings often perform at chance level in a ZeTT setting, we propose a new solution: we train a hypernetwork taking a tokenizer as input and predicting the corresponding embeddings. We empirically demonstrate that the hypernetwork generalizes to new tokenizers both with encoder (e.g., XLM-R) and decoder LLMs (e.g., Mistral-7B). Our method comes close to the original models' performance in cross-lingual and coding tasks while markedly reducing the length of the tokenized sequence. We also find that the remaining gap can be quickly closed by continued training on less than 1B tokens. Finally, we show that a ZeTT hypernetwork trained for a base (L)LM can also be applied to fine-tuned variants without extra training. Overall, our results make substantial strides toward detaching LMs from their tokenizer.","tags":null,"title":"Zero-Shot Tokenizer Transfer","type":"publication"},{"authors":["Piotr Nawrot","Adrian Łańcucki","Marcin Chochowski","David Tarjan","Edoardo M. Ponti"],"categories":null,"content":"","date":1710374400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710374400,"objectID":"36b7dd06cf50ab20a9f192eb1e4ddd82","permalink":"https://ducdauge.github.io/publication/2024-dmc/","publishdate":"2024-03-14T00:00:00Z","relpermalink":"/publication/2024-dmc/","section":"publication","summary":"Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for on-line key-value cache compression at inference time. Most importantly, the model learns to apply different compression rates in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to ~3.7x throughput increase in auto-regressive inference on a NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. We find that DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention (GQA). GQA and DMC can be even combined to obtain compounded gains. As a result DMC fits longer contexts and larger batches within any given memory budget.","tags":null,"title":"Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference","type":"publication"},{"authors":["Alan Ansell","Ivan Vulić","Hannah Sterz","Anna Korhonen","Edoardo M. Ponti"],"categories":null,"content":"","date":1706486400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1706486400,"objectID":"f76a19cf83e3a7f75f354777509a613a","permalink":"https://ducdauge.github.io/publication/2024-sft/","publishdate":"2024-01-29T00:00:00Z","relpermalink":"/publication/2024-sft/","section":"publication","summary":"Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with instructions or human feedback) due to their sheer number of parameters. A family of parameter-efficient sparse fine-tuning methods have proven promising in terms of performance but their memory requirements increase proportionally to the size of the LLMs. In this work, we scale sparse fine-tuning to state-of-the-art LLMs like LLaMA 2 7B and 13B. We propose SpIEL, a novel sparse fine-tuning method which, for a desired density level, maintains an array of parameter indices and the deltas of these parameters relative to their pretrained values. It iterates over: (a) updating the active deltas, (b) pruning indices (based on the change of magnitude of their deltas) and (c) regrowth of indices. For regrowth, we explore two criteria based on either the accumulated gradients of a few candidate parameters or their approximate momenta estimated using the efficient SM3 optimizer. We experiment with instruction-tuning of LLMs on standard dataset mixtures, finding that SpIEL is often superior to popular parameter-efficient fine-tuning methods like LoRA (low-rank adaptation) in terms of performance and comparable in terms of run time. We additionally show that SpIEL is compatible with both quantization and efficient optimizers, to facilitate scaling to ever-larger model sizes.","tags":null,"title":"Scaling Sparse Fine-Tuning to Large Language Models","type":"publication"},{"authors":["Edoardo M. Ponti","Alessandro Sordoni","Yoshua Bengio","Siva Reddy"],"categories":null,"content":"","date":1646006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646006400,"objectID":"6df633d08dbc423d3c03dc3287620be3","permalink":"https://ducdauge.github.io/publication/2022-polytropon/","publishdate":"2022-02-28T00:00:00Z","relpermalink":"/publication/2022-polytropon/","section":"publication","summary":"A modular design encourages neural models to disentangle and recombine different facets of knowledge to generalise more systematically to new tasks. In this work, we assume that each task is associated with a subset of latent skills from an (arbitrary size) inventory. In turn, each skill corresponds to a parameter-efficient (sparse / low-rank) model adapter. By jointly learning adapters and a routing function that allocates skills to each task, the full network is instantiated as the average of the parameters of active skills. We propose several inductive biases that encourage re-usage and composition of the skills, including variable-size skill allocation and a dual-speed learning rate. We evaluate our latent-skill model in two main settings: 1) multitask reinforcement learning for instruction following on 8 levels of the BabyAI platform; and 2) few-shot fine-tuning of language models on 160 NLP tasks of the CrossFit benchmark. We find that the modular design of our network enhances sample efficiency in reinforcement learning and few-shot generalisation in supervised learning, compared to a series of baselines. These include models where parameters are fully shared, task-specific, conditionally generated (HyperFormer), or sparse mixture-of-experts (TaskMoE).","tags":null,"title":"Combining Modular Skills in Multitask Learning","type":"publication"},{"authors":["Edoardo M. Ponti","Julia Kreutzer","Ivan Vulić","Siva Reddy"],"categories":null,"content":"","date":1636416000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636416000,"objectID":"5087ce20d0fb8f5bea2e0828a23c2f0c","permalink":"https://ducdauge.github.io/publication/2021-latent-translation/","publishdate":"2021-11-09T00:00:00Z","relpermalink":"/publication/2021-latent-translation/","section":"publication","summary":"While achieving state-of-the-art results in multiple tasks and languages, translation-based cross-lingual transfer is often overlooked in favour of massively multilingual pre-trained encoders. Arguably, this is due to its main limitations: 1) translation errors percolating to the classification phase and 2) the insufficient expressiveness of the maximum-likelihood translation. To remedy this, we propose a new technique that integrates both steps of the traditional pipeline (translation and classification) into a single model, by treating the intermediate translations as a latent random variable. As a result, 1) the neural machine translation system can be fine-tuned with a variant of Minimum Risk Training where the reward is the accuracy of the downstream task classifier. Moreover, 2) multiple samples can be drawn to approximate the expected loss across all possible translations during inference. We evaluate our novel latent translation-based model on a series of multilingual NLU tasks, including commonsense reasoning, paraphrase identification, and natural language inference. We report gains for both zero-shot and few-shot learning setups, up to 2.7 accuracy points on average, which are even more prominent for low-resource languages (e.g., Haitian Creole). Finally, we carry out in-depth analyses comparing different underlying NMT models and assessing the impact of alternative translations on the downstream performance.","tags":null,"title":"Modelling Latent Translations for Cross-Lingual Transfer","type":"publication"},{"authors":["Fangyu Liu","Emanuele Bugliarello","Edoardo M. Ponti","Siva Reddy","Nigel Collier","Desmond Elliott"],"categories":null,"content":"","date":1636416000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636416000,"objectID":"0ac720f1e2195ca314bcff5ac9e8930f","permalink":"https://ducdauge.github.io/publication/2021-marvl/","publishdate":"2021-11-09T00:00:00Z","relpermalink":"/publication/2021-marvl/","section":"publication","summary":"The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for {M}ulticultur{a}l {R}easoning over {V}ision and {L}anguage (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems.","tags":null,"title":"Visually Grounded Reasoning across Languages and Cultures","type":"publication"},{"authors":["Alan Ansell","Edoardo M. Ponti","Anna Korhonen","Ivan Vulić"],"categories":null,"content":"","date":1634169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634169600,"objectID":"6727d84642e47cad17238bec73f6ec67","permalink":"https://ducdauge.github.io/publication/2021-sparseft/","publishdate":"2021-10-14T00:00:00Z","relpermalink":"/publication/2021-sparseft/","section":"publication","summary":"Fine-tuning all parameters of a pre-trained model has become the mainstream approach for transfer learning. To increase its efficiency and prevent catastrophic forgetting and interference, techniques like adapters and sparse fine-tuning have been developed. Adapters are modular, as they can be combined to adapt a model towards different facets of knowledge (e.g., dedicated language and/or task adapters). Sparse fine-tuning is expressive, as it controls the behavior of all model components. In this work, we introduce a new fine-tuning method with both these desirable properties. In particular, we learn sparse, real-valued masks based on a simple variant of the Lottery Ticket Hypothesis. Task-specific masks are obtained from annotated data in a source language, and language-specific masks from masked language modeling in a target language. Both these masks can then be composed with the pre-trained model. Unlike adapter-based fine-tuning, this method neither increases the number of parameters at inference time nor alters the original model architecture. Most importantly, it outperforms adapters in zero-shot cross-lingual transfer by a large margin in a series of multilingual benchmarks, including Universal Dependencies, MasakhaNER, and AmericasNLI. Based on an in-depth analysis, we additionally find that sparsity is crucial to prevent both 1) interference between the fine-tunings to be composed and 2) overfitting. We release the code and models at https://github.com/cambridgeltl/composable-sft.","tags":null,"title":"Composable Sparse Fine-Tuning for Cross-Lingual Transfer","type":"publication"},{"authors":["Edoardo M. Ponti","Goran Glavaš","Olga Majewska","Qianchu Liu","Ivan Vulić","Anna Korhonen"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"6f3187d13bb16b0da61dabe9c81ae1b4","permalink":"https://ducdauge.github.io/publication/2020-xcopa/","publishdate":"2020-05-01T00:00:00Z","relpermalink":"/publication/2020-xcopa/","section":"publication","summary":"In order to simulate human language capacity, natural language processing systems must complement the explicit information derived from raw text with the ability to reason about the possible causes and outcomes of everyday situations. Moreover, the acquired world knowledge should generalise to new languages, modulo cultural differences. Advances in machine commonsense reasoning and cross-lingual transfer depend on the availability of challenging evaluation benchmarks. Motivated by both demands, we introduce Cross-lingual Choice of Plausible Alternatives (XCOPA), a typologically diverse multilingual dataset for causal commonsense reasoning in 11 languages. We benchmark a range of state-of-the-art models on this novel dataset, revealing that current methods based on multilingual pretraining and zero-shot fine-tuning transfer suffer from the curse of multilinguality and fall short of performance in monolingual settings by a large margin. Finally, we propose ways to adapt these models to out-of-sample resource-lean languages where only a small corpus or a bilingual dictionary is available, and report substantial improvements over the random baseline. XCOPA is available at github.com/cambridgeltl/xcopa.","tags":null,"title":"XCOPA: A Multilingual Dataset for Causal Commonsense Reasoning","type":"publication"},{"authors":["Ivan Vulić","Simon Baker","Edoardo Maria Ponti","Ulla Petti","Ira Leviant","Kelly Wing","Olga Majewska","Eden Bar","Matt Malone","Thierry Poibeau","Roi Reichart","Anna Korhonen"],"categories":null,"content":"","date":1583798400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583798400,"objectID":"5f016cdd2f71f4c105c50d0c4035c22d","permalink":"https://ducdauge.github.io/publication/2020-multi-simlex/","publishdate":"2020-03-10T00:00:00Z","relpermalink":"/publication/2020-multi-simlex/","section":"publication","summary":"We introduce Multi-SimLex, a large-scale lexical resource and evaluation benchmark covering datasets for 12 typologically diverse languages, including major languages (e.g., Mandarin Chinese, Spanish, Russian) as well as less-resourced ones (e.g., Welsh, Kiswahili). Each language dataset is annotated for the lexical relation of semantic similarity and contains 1,888 semantically aligned concept pairs, providing a representative coverage of word classes (nouns, verbs, adjectives, adverbs), frequency ranks, similarity intervals, lexical fields, and concreteness levels. Additionally, owing to the alignment of concepts across languages, we provide a suite of 66 cross-lingual semantic similarity datasets. Due to its extensive size and language coverage, Multi-SimLex provides entirely novel opportunities for experimental evaluation and analysis. On its monolingual and cross-lingual benchmarks, we evaluate and analyze a wide array of recent state-of-the-art monolingual and cross-lingual representation models, including static and contextualized word embeddings (such as fastText, M-BERT and XLM), externally informed lexical representations, as well as fully unsupervised and (weakly) supervised cross-lingual word embeddings. We also present a step-by-step dataset creation protocol for creating consistent, Multi-Simlex-style resources for additional languages. We make these contributions -- the public release of Multi-SimLex datasets, their creation protocol, strong baseline results, and in-depth analyses which can be be helpful in guiding future developments in multilingual lexical semantics and representation learning -- available via a website which will encourage community effort in further expansion of Multi-Simlex to many more languages. Such a large-scale semantic resource could inspire significant further advances in NLP across languages.","tags":null,"title":"Multi-SimLex: A Large-Scale Evaluation of Multilingual and Cross-Lingual Lexical Semantic Similarity","type":"publication"},{"authors":["Edoardo M. Ponti","Ivan Vulić","Ryan Cotterell","Marinela Parovic","Roi Reichart","Anna Korhonen"],"categories":null,"content":"","date":1580342400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580342400,"objectID":"4e5771a86570d714cd83ed82c0c1b06f","permalink":"https://ducdauge.github.io/publication/2020-parameter-space/","publishdate":"2020-01-30T00:00:00Z","relpermalink":"/publication/2020-parameter-space/","section":"publication","summary":"Most combinations of NLP tasks and language varieties lack in-domain examples for supervised training because of the paucity of annotated data. How can neural models make sample-efficient generalizations from task-language combinations with available data to low-resource ones? In this work, we propose a Bayesian generative model for the space of neural parameters. We assume that this space can be factorized into latent variables for each language and each task. We infer the posteriors over such latent variables based on data from seen task-language combinations through variational inference. This enables zero-shot classification on unseen combinations at prediction time. For instance, given training data for named entity recognition (NER) in Vietnamese and for part-of-speech (POS) tagging in Wolof, our model can perform accurate predictions for NER in Wolof. In particular, we experiment with a typologically diverse sample of 33 languages from 4 continents and 11 families, and show that our model yields comparable or better results than state-of-the-art, zero-shot cross-lingual transfer methods; it increases performance by 4.49 points for POS tagging and 7.73 points for NER on average compared to the strongest baseline.","tags":null,"title":"Parameter Space Factorization for Zero-Shot Learning across Tasks and Languages","type":"publication"},{"authors":["Edoardo M. Ponti","Ivan Vulić","Goran Glavaš","Roi Reichart","Anna Korhonen"],"categories":null,"content":"","date":1572739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572739200,"objectID":"190e8248c4b2870af3a50f5a80ef3a4a","permalink":"https://ducdauge.github.io/publication/2019-crosslingual-specialization/","publishdate":"2019-11-03T00:00:00Z","relpermalink":"/publication/2019-crosslingual-specialization/","section":"publication","summary":"Semantic specialization integrates structured linguistic knowledge from external resources (such as lexical relations in WordNet) into pretrained distributional vectors in the form of constraints. However, this technique cannot be leveraged in many languages, because their structured external resources are typically incomplete or non-existent. To bridge this gap, we propose a novel method that transfers specialization from a resource-rich source language (English) to virtually any target language. Our specialization transfer comprises two crucial steps: 1) Inducing noisy constraints in the target language through automatic word translation; and 2) Filtering the noisy constraints via a state-of-the-art relation prediction model trained on the source language constraints. This allows us to specialize any set of distributional vectors in the target language with the refined constraints. We prove the effectiveness of our method through intrinsic word similarity evaluation in 8 languages, and with 3 downstream tasks in 5 languages: lexical simplification, dialog state tracking, and semantic textual similarity. The gains over the previous state-of-art specialization methods are substantial and consistent across languages. Our results also suggest that the transfer method is effective even for lexically distant source-target language pairs. Finally, as a by-product, our method produces lists of WordNet-style lexical relations in resource-poor languages.","tags":null,"title":"Cross-lingual Semantic Specialization via Lexical Relation Induction","type":"publication"},{"authors":["Edoardo M. Ponti","Ivan Vulić","Ryan Cotterell","Roi Reichart","Anna Korhonen"],"categories":null,"content":"","date":1572739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572739200,"objectID":"5159daa7fe88975a70852a5570d60987","permalink":"https://ducdauge.github.io/publication/2019-towards-zero/","publishdate":"2019-11-03T00:00:00Z","relpermalink":"/publication/2019-towards-zero/","section":"publication","summary":"Can we construct a neural language model which is inductively biased towards learning human language? Motivated by this question, we aim at constructing an informative prior for held-out languages on the task of character-level, open-vocabulary language modeling. We obtain this prior as the posterior over network weights conditioned on the data from a sample of training languages, which is approximated through Laplace’s method. Based on a large and diverse sample of languages, the use of our prior outperforms baseline models with an uninformative prior in both zero-shot and few-shot settings, showing that the prior is imbued with universal linguistic knowledge. Moreover, we harness broad language-specific information available for most languages of the world, ie, features from typological databases, as distant supervision for held-out languages. We explore several language modeling conditioning techniques, including concatenation and metanetworks for parameter generation. They appear beneficial in the few-shot setting, but ineffective in the zero-shot setting. Since the paucity of even plain digital text affects the majority of the world’s languages, we hope that these insights will broaden the scope of applications for language technology.","tags":null,"title":"Towards Zero-shot Language Modeling","type":"publication"},{"authors":["Anne Lauscher","Ivan Vulić","Edoardo Maria Ponti","Anna Korhonen","Goran Glavaš"],"categories":null,"content":"","date":1567641600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567641600,"objectID":"85ca1918baa3c9f43dc60dc731adb6c9","permalink":"https://ducdauge.github.io/publication/2019-informing-unsupervised/","publishdate":"2019-09-05T00:00:00Z","relpermalink":"/publication/2019-informing-unsupervised/","section":"publication","summary":"Unsupervised pretraining models have been shown to facilitate a wide range of downstream applications. These models, however, still encode only the distributional knowledge, incorporated through language modeling objectives. In this work, we complement the encoded distributional knowledge with external lexical knowledge. We generalize the recently proposed (state-of-the-art) unsupervised pretraining model BERT to a multi-task learning setting: we couple BERT's masked language modeling and next sentence prediction objectives with the auxiliary binary word relation classification, through which we inject clean linguistic knowledge into the model. Our initial experiments suggest that our \"linguistically-informed\" BERT (LIBERT) yields performance gains over the linguistically-blind \"vanilla\" BERT on several language understanding tasks.","tags":null,"title":"Informing Unsupervised Pretraining with External Linguistic Knowledge","type":"publication"},{"authors":["Aishwarya Kamath","Jonas Pfeiffer","Edoardo Ponti","Goran Glavaš","Ivan Vulić"],"categories":null,"content":"","date":1564704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564704000,"objectID":"efe11af63f5d415ccd4afadd61bc73bb","permalink":"https://ducdauge.github.io/publication/2019-specializing-all/","publishdate":"2019-08-02T00:00:00Z","relpermalink":"/publication/2019-specializing-all/","section":"publication","summary":"Semantic specialization methods fine-tune distributional word vectors using lexical knowledge from external resources (e.g., WordNet) to accentuate a particular relation between words. However, such post-processing methods suffer from limited coverage as they affect only vectors of words seen in the external resources. We present the first postprocessing method that specializes vectors of all vocabulary words – including those unseen in the resources – for the asymmetric relation of lexical entailment (LE) (i.e., hyponymyhypernymy relation). Leveraging a partially LE-specialized distributional space, our POSTLE (i.e., post-specialization for LE) model learns an explicit global specialization function, allowing for specialization of vectors of unseen words, as well as word vectors from other languages via cross-lingual transfer. We capture the function as a deep feedforward neural network: its objective re-scales vector norms to reflect the concept hierarchy while simultaneously attracting hyponymyhypernymy pairs to better reflect semantic similarity. An extended model variant augments the basic architecture with an adversarial discriminator. We demonstrate the usefulness and versatility of POSTLE models with different input distributional spaces in different scenarios (monolingual LE and zero-shot cross-lingual LE transfer) and tasks (binary and graded LE). We report consistent gains over state-of-the-art LE-specialization methods, and successfully LE-specialize word vectors for languages without any external lexical knowledge.","tags":null,"title":"Specializing Distributional Vectors of All Words for Lexical Entailment","type":"publication"},{"authors":["Edoardo Maria Ponti","Helen O’Horan","Yevgeni Berzak","Ivan Vulić","Roi Reichart","Thierry Poibeau","Ekaterina Shutova","Anna Korhonen"],"categories":null,"content":"","date":1549929600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549929600,"objectID":"cd6391dd003be2146672366475a2ea00","permalink":"https://ducdauge.github.io/publication/2019-modeling-language/","publishdate":"2019-02-12T00:00:00Z","relpermalink":"/publication/2019-modeling-language/","section":"publication","summary":"Linguistic typology aims to capture structural and semantic variation across the world’s languages. A large-scale typology could provide excellent guidance for multilingual Natural Language Processing (NLP), particularly for languages that suffer from the lack of human labeled resources. We present an extensive literature survey on the use of typological information in the development of NLP techniques. Our survey demonstrates that to date, the use of information in existing typological databases has resulted in consistent but modest improvements in system performance. We show that this is due to both intrinsic limitations of databases (in terms of coverage and feature granularity) and under-utilization of the typological features included in them. We advocate for a new approach that adapts the broad and discrete nature of typological categories to the contextual and continuous nature of machine learning algorithms used in contemporary NLP. In particular, we suggest that such an approach could be facilitated by recent developments in data-driven induction of typological knowledge.","tags":null,"title":"Modeling Language Variation and Universals: A Survey on Typological Linguistics for Natural Language Processing","type":"publication"},{"authors":["Edoardo M. Ponti","Ivan Vulić","Goran Glavaš","Nikola Mrkšić","Anna Korhonen"],"categories":null,"content":"","date":1540598400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540598400,"objectID":"03ee6c487697bd49777c4adcbd6280f3","permalink":"https://ducdauge.github.io/publication/2018-adversarial-propagation/","publishdate":"2018-10-27T00:00:00Z","relpermalink":"/publication/2018-adversarial-propagation/","section":"publication","summary":"Semantic specialization is the process of fine-tuning pre-trained distributional word vectors using external lexical knowledge (eg, WordNet) to accentuate a particular semantic relation in the specialized vector space. While post-processing specialization methods are applicable to arbitrary distributional vectors, they are limited to updating only the vectors of words occurring in external lexicons (ie, seen words), leaving the vectors of all other words unchanged. We propose a novel approach to specializing the full distributional vocabulary. Our adversarial post-specialization method propagates the external lexical knowledge to the full distributional space. We exploit words seen in the resources as training examples for learning a global specialization function. This function is learned by combining a standard L2-distance loss with an adversarial loss: the adversarial component produces more realistic output vectors. We show the effectiveness and robustness of the proposed method across three languages and on three tasks: word similarity, dialog state tracking, and lexical simplification. We report consistent improvements over distributional word vectors and vectors specialized by other state-of-the-art specialization frameworks. Finally, we also propose a cross-lingual transfer method for zero-shot specialization which successfully specializes a full target distributional space without any lexical knowledge in the target language and without any bilingual data.","tags":null,"title":"Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word Vector Specialization","type":"publication"},{"authors":null,"categories":null,"content":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"53e892b8b41cc4caece1cfd5ef21d6e7","permalink":"https://ducdauge.github.io/license/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/license/","section":"","summary":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.","tags":null,"title":"LICENSE: CC-BY-SA","type":"page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"db9c7cbc7ea39f900c5763526a87142d","permalink":"https://ducdauge.github.io/faq/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/faq/","section":"","summary":"","tags":null,"title":"Frequently Asked Questions","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"65de3680a280f6bf29dc34fe1adad5a6","permalink":"https://ducdauge.github.io/talks/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/talks/","section":"","summary":"","tags":null,"title":"Talks","type":"widget_page"},{"authors":null,"categories":null,"content":"\rTweets by PontiEdoardo ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"dd3ba838feb0ecb8c6fa818ca89c2e80","permalink":"https://ducdauge.github.io/twitter/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/twitter/","section":"","summary":"\rTweets by PontiEdoardo ","tags":null,"title":"Twitter feed","type":"page"}]