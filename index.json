[{"authors":["emponti"],"categories":null,"content":"I am a third-year Ph.D. student in Computational Linguistics at the University of Cambridge, where I am affiliated to St John\u0026rsquo;s College. I am supervised by Prof. Anna Korhonen and funded through the ERC project LEXICAL. I have previously interned as an AI/ML researcher at Apple in Cupertino, and won a Google Research Faculty Award for a project co-written with my supervisor.\nMost of my research revolves around the inductive bias of artificial neural networks towards language, in order to enable few-shot learning. Moreover, I am interested in integrating text-based word representations with world and lexical knowledge.\nI am a literature enthusiast, a decently bad violinist and a cat lover.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"213193b58a0bdd6832138a4f61012d29","permalink":"https://ducdauge.github.io/authors/emponti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/emponti/","section":"authors","summary":"I am a third-year Ph.D. student in Computational Linguistics at the University of Cambridge, where I am affiliated to St John\u0026rsquo;s College. I am supervised by Prof. Anna Korhonen and funded through the ERC project LEXICAL. I have previously interned as an AI/ML researcher at Apple in Cupertino, and won a Google Research Faculty Award for a project co-written with my supervisor.\nMost of my research revolves around the inductive bias of artificial neural networks towards language, in order to enable few-shot learning.","tags":null,"title":"Edoardo M. Ponti","type":"authors"},{"authors":["Edoardo M. Ponti","Ivan Vulić","Ryan Cotterell","Roi Reichart","Anna Korhonen"],"categories":null,"content":"","date":1572739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572739200,"objectID":"ad583806e19b8823a1d32e42aa2cc88d","permalink":"https://ducdauge.github.io/publication/2019-towards-zero/","publishdate":"2019-11-03T00:00:00Z","relpermalink":"/publication/2019-towards-zero/","section":"publication","summary":"Can we construct a neural language model which is inductively biased towards learning human language? Motivated by this question, we aim at constructing an informative prior for held-out languages on the task of character-level, open-vocabulary language modeling. We obtain this prior as the posterior over network weights conditioned on the data from a sample of training languages, which is approximated through Laplace’s method. Based on a large and diverse sample of languages, the use of our prior outperforms baseline models with an uninformative prior in both zero-shot and few-shot settings, showing that the prior is imbued with universal linguistic knowledge. Moreover, we harness broad language-specific information available for most languages of the world, ie, features from typological databases, as distant supervision for held-out languages. We explore several language modeling conditioning techniques, including concatenation and metanetworks for parameter generation. They appear beneficial in the few-shot setting, but ineffective in the zero-shot setting. Since the paucity of even plain digital text affects the majority of the world’s languages, we hope that these insights will broaden the scope of applications for language technology.","tags":null,"title":"Towards Zero-shot Language Modeling","type":"publication"},{"authors":["Aishwarya Kamath","Jonas Pfeiffer","Edoardo Ponti","Goran Glavaš","Ivan Vulić"],"categories":null,"content":"","date":1564704000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564704000,"objectID":"4e401978c5b9b040a7f678822315554f","permalink":"https://ducdauge.github.io/publication/2019-specializing-all/","publishdate":"2019-08-02T00:00:00Z","relpermalink":"/publication/2019-specializing-all/","section":"publication","summary":"Semantic specialization methods fine-tune distributional word vectors using lexical knowledge from external resources (e.g., WordNet) to accentuate a particular relation between words. However, such post-processing methods suffer from limited coverage as they affect only vectors of words seen in the external resources. We present the first postprocessing method that specializes vectors of all vocabulary words – including those unseen in the resources – for the asymmetric relation of lexical entailment (LE) (i.e., hyponymyhypernymy relation). Leveraging a partially LE-specialized distributional space, our POSTLE (i.e., post-specialization for LE) model learns an explicit global specialization function, allowing for specialization of vectors of unseen words, as well as word vectors from other languages via cross-lingual transfer. We capture the function as a deep feedforward neural network: its objective re-scales vector norms to reflect the concept hierarchy while simultaneously attracting hyponymyhypernymy pairs to better reflect semantic similarity. An extended model variant augments the basic architecture with an adversarial discriminator. We demonstrate the usefulness and versatility of POSTLE models with different input distributional spaces in different scenarios (monolingual LE and zero-shot cross-lingual LE transfer) and tasks (binary and graded LE). We report consistent gains over state-of-the-art LE-specialization methods, and successfully LE-specialize word vectors for languages without any external lexical knowledge.","tags":null,"title":"Specializing Distributional Vectors of All Words for Lexical Entailment","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"https://ducdauge.github.io/projects/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"See some of the projects I have worked on","tags":null,"title":"Projects","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"65de3680a280f6bf29dc34fe1adad5a6","permalink":"https://ducdauge.github.io/talks/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/talks/","section":"","summary":"Upcoming and recent talks / workshops","tags":null,"title":"Talks \u0026 Workshops","type":"widget_page"},{"authors":["Edoardo M. Ponti","Ivan Vulić","Goran Glavaš","Nikola Mrkšić","Anna Korhonen"],"categories":null,"content":"","date":1540598400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540598400,"objectID":"443a4867ee3d9ffc333dc7bfd5a7ef7a","permalink":"https://ducdauge.github.io/publication/2018-adversarial-propagation/","publishdate":"2018-10-27T00:00:00Z","relpermalink":"/publication/2018-adversarial-propagation/","section":"publication","summary":"Semantic specialization is the process of fine-tuning pre-trained distributional word vectors using external lexical knowledge (eg, WordNet) to accentuate a particular semantic relation in the specialized vector space. While post-processing specialization methods are applicable to arbitrary distributional vectors, they are limited to updating only the vectors of words occurring in external lexicons (ie, seen words), leaving the vectors of all other words unchanged. We propose a novel approach to specializing the full distributional vocabulary. Our adversarial post-specialization method propagates the external lexical knowledge to the full distributional space. We exploit words seen in the resources as training examples for learning a global specialization function. This function is learned by combining a standard L2-distance loss with an adversarial loss: the adversarial component produces more realistic output vectors. We show the effectiveness and robustness of the proposed method across three languages and on three tasks: word similarity, dialog state tracking, and lexical simplification. We report consistent improvements over distributional word vectors and vectors specialized by other state-of-the-art specialization frameworks. Finally, we also propose a cross-lingual transfer method for zero-shot specialization which successfully specializes a full target distributional space without any lexical knowledge in the target language and without any bilingual data.","tags":null,"title":"Adversarial Propagation and Zero-Shot Cross-Lingual Transfer of Word Vector Specialization","type":"publication"},{"authors":null,"categories":null,"content":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n  \n","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"53e892b8b41cc4caece1cfd5ef21d6e7","permalink":"https://ducdauge.github.io/license/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/license/","section":"","summary":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.","tags":null,"title":"LICENSE: CC-BY-SA","type":"page"}]